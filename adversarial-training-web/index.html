<!DOCTYPE html>
<meta charset="utf-8">

<head>
	<title>Exploring Adversarial Training Strategies with PGD Attack</title>

	<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
	<link rel="stylesheet" type="text/css" href="css/view.css">
	<link rel="stylesheet" type="text/css" href="css/style.css">
	<link rel="stylesheet" type="text/css" href="css/font.css">
	<link rel="stylesheet" type="text/css" href="fonts/Font-Awesome/css/all.min.css">

	<script src="https://code.jquery.com/jquery-3.4.1.slim.min.js" integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n" crossorigin="anonymous"></script>
	<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
	<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
	<script src="vendor/d3.v5.min.js"></script>
</head>

<body>

	<div id="header" class="section">
		<div class="title">Exploring Adversarial Training Strategies with PGD Attack</div>
		<div class="introduction">

			<div>Deep neural networks are powerful in image classification but are vulnerable to adversarial attacks—subtle perturbations to images can lead to incorrect classifications. A well-known L∞-bounded adversarial attack <a href="#l-inf-bound">[1]</a> is the projected gradient descent (PGD) attack <a href="#pgd">[2]</a>. It was shown that PGD adversarial training (i.e. producing adversarial examples using PGD and train a deep neural network using the adversarial examples) improves model resistance to a wide range of attacks <a href="#pgd">[2]</a>.</div>

			<div>A goal of adversarial training is to ensure that the model is robust to the adversarial examples (i.e. the model should have a low adversarial error) while not harming the performance on the benign examples (i.e. the model should have a low error on the test set).</div>

			<div>In this article, we explore three adversarial training strategies. For each strategy, we conducted six experiments by training six models. Our goal is to understaning which strategy can produce both a low adversarial error and a low error on the test set.</div>

			<div>For each of the 18 experiments, we trained a VGG16 model on the CIFAR10 dataset with a learning rate of 0.1 and employed a PGD attack where Ɛ equals 0.05. Each pixel in an image is normalized between 0 and 1. We trained all the 18 models for 50 epochs.</div>

		</div>
	</div>

	<div id="stochastic">
		<div class="title">Strategy 1: Stochastic Adversarial Attack</div>
		<div class="introduction">
			
			<div>For this strategy, we trained VGG16 models on the 50,000 CIFAR10 training images, each having a certain probability of being perturbed by PGD. With six attack probabilities (0%, 20%, 40%, 60%, 80%, 100%), we trained six VGG16 models.</div>

			<div>When the attack probability is 0%, the model is trained on 50,000 benign examples. When the attack probability is 60%, the model is trained on approximately 60% x 50,000 adversarial examples and approximately 40% x 50,000 benign examples.</div>

			<div>We trained each model for 50 epochs. After each epoch, we evaluated the errors on the benign training set (<span class="tr-e">train_err</span>), the benign test set (<span class="te-e">test_err</span>), and the adversarial test set (<span class="adv-e">adv_err</span>). The following graph shows the three errors at the 50th epoch for each model.</div>

			<div><a href="https://github.com/terrancelaw/terrancelaw.github.io/blob/master/adversarial-training-experiments/stochastic/stochastic.ipynb" target="_blank">Jupyter Notebook for the Experiments</a></div>

		</div>
		<div class="vis-container">
			<span class="chart"><svg></svg></span>
			<span class="finding">
				
				<div>Key Observations:</div>

				<div>1. The model trained only on benign examples (attack probability=0%) has the lowest <span class="te-e">test_err</span> and <span class="adv-e">adv_err</span>.</div>

				<div>2. Surprisingly, <span class="adv-e">adv_err</span> increases drastically as the attack probability increases from 0% to 20% .</div>

				<div>3. As the attack probability increases, <span class="adv-e">adv_err</span> decreases while <span class="te-e">test_err</span> increases.</div>

			</span>
		</div>
	</div>

	<div id="learning-from-clean">
		<div class="title">Strategy 2: Train on Benign Examples, Then on Adversarial Examples</div>
		<div class="introduction">
			
			<div>For the second strategy, we first trained a VGG16 model on the 50,000 benign examples. We then continued the training on the same 50,000 training examples. However, only with a certain probability was an image used for training. If an image was used for training, it was first perturbed by PGD before the forward pass of backpropagation.</div>

			<div>For example, when the probability is 60%, there is 60% chance that an image is perturbed and used for training. In other words, the model is trained on approximately 60% x 50,000 adversarial examples (after training on the 50,000 benign examples).</div>

			<div>Using this strategy, we trained six models. The first is a VGG16 model trained on the 50,000 benign examples. The other five models were created by continuing the training on approximately 20%, 40%, 60%, 80%, and 100% x 50,000 adversarial examples.</div>

			<div><a href="https://github.com/terrancelaw/terrancelaw.github.io/blob/master/adversarial-training-experiments/transfer_from_clean/transfer_from_clean.ipynb" target="_blank">Jupyter Notebook for the Experiments</a></div>

		</div>
		<div class="vis-container">
			<span class="chart"><svg></svg></span>
			<span class="finding">
				
				<div>Key Observations:</div>

				<div>1. The model trained only on benign examples again has the lowest <span class="te-e">test_err</span> and the lowest <span class="adv-e">adv_err</span>.</div>

				<div>2. As the probability increases (training on more adversarial examples), <span class="te-e">test_err</span> decreases. This indicates that training on more adversarial examples helps the model generalize to benign examples.</div>

				<div>3. As the probability increases, <span class="adv-e">adv_err</span> mildly decreases.</div>

			</span>
		</div>
	</div>

	<div id="learning-from-adversarial">
		<div class="title">Strategy 3: Train on Adversarial Examples, Then on Benign Examples</div>
		<div class="introduction">
			
			<div>For the third strategy, we first trained a VGG16 model on the 50,000 adversarial examples. We then continued the training on the same 50,000 training examples. Again, an image was used for training only with a certain probability. Contrary to strategy 2, the images used for training are NOT perturbed.</div>

			<div>For example, when the probability is 60%, there is 60% chance that an image is used for training, and this image is NOT perturbed. In other words, the model is trained on approximately 60% x 50,000 benign examples (after training on the 50,000 adversarial examples).</div>

			<div>Using this strategy, we trained six models. The first is a VGG16 model trained on the 50,000 adversarial examples. The other five models were created by continuing the training on approximately 20%, 40%, 60%, 80%, and 100% x 50,000 benign examples.</div>

			<div><a href="https://github.com/terrancelaw/terrancelaw.github.io/blob/master/adversarial-training-experiments/transfer_from_adversarial/transfer_from_adversarial.ipynb" target="_blank">Jupyter Notebook for the Experiments</a></div>

		</div>
		<div class="vis-container">
			<span class="chart"><svg></svg></span>
			<span class="finding">
				
				<div>Key Observations:</div>

				<div>1. As the probability increases (training on more benign examples), <span class="te-e">test_err</span> decreases. Obviously, training on more benign examples helps the model generalize to benign examples.</div>

				<div>2. As the probability increases, <span class="adv-e">adv_err</span> also decreases, indicating that training on more benign examples helps the model generalize to adversarial examples.</div>

			</span>
		</div>
	</div>

	<div id="conclusion">
		<div class="title">Conclusion</div>
		<div class="introduction">
			
			<div>Unsurprisingly, the above findings suggest that training on benign examples reduces both error on adversarial examples (<span class="adv-e">adv_err</span> in Strategy 3 is downward trending) and error on benign examples (<span class="te-e">test_err</span> in Strategy 3 is downward trending). However, we also observe several surprising results.</div>

			<div>First, in Strategy 1, <span class="adv-e">adv_err</span> increases drastically as the attack probability increases from 0% to 20%. This is contrary to the observation that training on more adversarial examples reduces <span class="adv-e">adv_err</span> (<span class="adv-e">adv_err</span> in Strategy 1 is downward trending).</div>

			<div>Second, the trends of <span class="te-e">test_err</span> in Strategy 1 and Strategy 2 seem to offer contradictory conclusions: The upward trend of <span class="te-e">test_err</span> in Strategy 1 suggests that training on more adversarial examples harms the performance on benign examples while the downward trend of <span class="te-e">test_err</span> in Strategy 2 indicates that training on more adversarial examples helps the model generalize to benign examples.</div>

			<div>Finally, the best <span class="te-e">test_err</span> and <span class="adv-e">adv_err</span> were achieved by the model trained solely on the benign examples (see Strategy 1 and Strategy 2). While prior work claimed the robustness of models trained on PGD-made adversarial examples, our findings say otherwise—natural training produces the best <span class="te-e">test_err</span> and <span class="adv-e">adv_err</span>.</div>

			<div>To provide explanations for the quandary, further experiments (e.g., using a different Ɛ) will be required.</div>
		</div>
	</div>

	<div id="individual-experiment">
		<div class="title">Visualizing Results for Each Experiment</div>
		<div class="introduction">

			<div>Here is an interactive visualization that allows you to explore <span class="tr-e">train_err</span>, <span class="te-e">test_err</span>, and <span class="adv-e">adv_err</span> per epoch for each of the 18 experiments.</div>

			<span class="dropdown-name">Strategy: </span>
			
			<span class="dropdown">
				<button class="btn btn-secondary dropdown-toggle" type="button" id="dropdownMenuButton" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Stochastic Adversarial Attack</button>
				<div class="dropdown-menu" aria-labelledby="dropdownMenuButton">
					<a class="dropdown-item selected" value="1">Stochastic Adversarial Attack</a>
					<a class="dropdown-item" value="2">Benign First, Then Adversarial</a>
					<a class="dropdown-item" value="3">Adversarial First, Then Benign</a>
		  		</div>
			</span>

			<span class="dropdown-name">Experiment: </span>

			<span class="dropdown">
				<button class="btn btn-secondary dropdown-toggle" type="button" id="dropdownMenuButton" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">0%</button>
				<div class="dropdown-menu" aria-labelledby="dropdownMenuButton">
					<a class="dropdown-item selected" value="1">0%</a>
					<a class="dropdown-item" value="2">20%</a>
					<a class="dropdown-item" value="3">40%</a>
					<a class="dropdown-item" value="4">60%</a>
					<a class="dropdown-item" value="5">80%</a>
					<a class="dropdown-item" value="6">100%</a>
		  		</div>
			</span>

		</div>
		<div class="vis-container">
			<span class="chart"><svg></svg></span>
			<span class="finding"></span>
		</div>
	</div>

	<div id="reference">
		<div>
			<a name="l-inf-bound"></a>[1] Know Your Adversary: Understanding Adversarial Examples (Part 1/2). (<a href="https://towardsdatascience.com/know-your-adversary-understanding-adversarial-examples-part-1-2-63af4c2f5830" target="_blank">https://towardsdatascience.com/know-your-adversary-understanding-adversarial-examples-part-1-2-63af4c2f5830</a>)
		</div>
		<div>
			<a name="pgd"></a>[2] Towards Deep Learning Models Resistant to Adversarial Attacks. (<a href="https://arxiv.org/pdf/1706.06083.pdf" target="_blank">https://arxiv.org/pdf/1706.06083.pdf</a>)
		</div>
	</div>

	<script src="js/start.js"></script>
	<script src="js/database.js"></script>
	<script src="js/stochasticView.js"></script>
	<script src="js/learningFromCleanView.js"></script>
	<script src="js/learningFromAdvView.js"></script>
	<script src="js/individualExperimentView.js"></script>
</body>