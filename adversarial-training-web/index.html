<!DOCTYPE html>
<meta charset="utf-8">

<head>
	<title>Exploring Adversarial Training Strategies for Weak PGD Attack</title>

	<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
	<link rel="stylesheet" type="text/css" href="css/view.css">
	<link rel="stylesheet" type="text/css" href="css/style.css">
	<link rel="stylesheet" type="text/css" href="css/font.css">
	<link rel="stylesheet" type="text/css" href="fonts/Font-Awesome/css/all.min.css">

	<script src="https://code.jquery.com/jquery-3.4.1.slim.min.js" integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n" crossorigin="anonymous"></script>
	<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
	<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
	<script src="vendor/d3.v5.min.js"></script>
</head>

<body>

	<div id="header" class="section">
		<div class="title">Exploring Adversarial Training Strategies for Weak PGD Attack</div>
		<div class="introduction">

			<div>Deep neural networks are powerful in image classification but are vulnerable to adversarial attacks—subtle perturbations to images can lead to incorrect classifications. A well-known adversarial attack is projected gradient descent (PGD) <a href="#pgd">[1]</a>. It was shown that PGD adversarial training (i.e. producing adversarial examples using PGD and train a deep neural network using the attacked examples) improves model resistance to a wide range of attacks <a href="#pgd">[1]</a>.</div>

			<div>A goal of adversarial training is to ensure that the model is robust to the adversarial examples (i.e. the model should have low adversarial error) while not harming the performance on the benign examples (i.e. the model should have low error on the test set).</div>

			<div>In this article, we explore three adversarial training strategies. For each strategy, we conducted six experiments by training six models. Our goal is to understaning which strategy can produce both a low adversarial error and a low low error on the test set.</div>

			<div>For each of the 18 experiments, we trained a VGG16 (lr=0.1) on the CIFAR10 dataset and employed a weak PGD attack (Ɛ=0.05). We trained all the 18 models for 50 epochs.</div>

		</div>
	</div>

	<div id="stochastic">
		<div class="title">Strategy 1: Stochastic Adversarial Attack</div>
		<div class="introduction">
			
			<div>We trained VGG16 models on the 50,000 CIFAR10 training examples. For this strategy, there is a certain probability that an image is perturbed using PGD. With six attack probabilities (0%, 20%, 40%, 60%, 80%, 100%), we trained six VGG16 models.</div>

			<div>When the attack probability is 0%, the model is trained on 50,000 benign examples. When the attack probability is 60%, the model is trained on approximately 60% x 50,000 adversarial examples.</div>

			<div>We trained each model for 50 epochs. After each epoch, we evaluate the error on the benign training set (<span class="tr-e">train_err</span>), the benign test set (<span class="te-e">test_err</span>), and the adversarial test set (<span class="adv-e">adv_err</span>). The following graph shows the three errors at the 50th epoch for each model.</div>

			<div><a href="https://github.com/terrancelaw/terrancelaw.github.io/blob/master/adversarial-training-experiments/stochastic/stochastic.ipynb" target="_blank">Jupyter Notebook for the Experiments</a></div>

		</div>
		<div class="vis-container">
			<span class="chart"><svg></svg></span>
			<span class="finding">
				
				<div>Key Observations:</div>

				<div>1. The model trained only on benign examples (attack probability=0%) has the lowest <span class="te-e">test_err</span>. Interestingly, it also has the lowest <span class="adv-e">adv_err</span>.</div>

				<div>2. As the attack probability increases, <span class="adv-e">adv_err</span> decreases. Obviously, training on more adversarial examples helps the model generalize to adversarial examples.</div>

				<div>3. As the attack probability increases, <span class="te-e">test_err</span> increases.</div>

			</span>
		</div>
	</div>

	<div id="learning-from-clean">
		<div class="title">Strategy 2: Train on Benign Examples, Then on Adversarial Examples</div>
		<div class="introduction">
			
			<div>For the second strategy, we first trained a VGG16 model on the 50,000 benign examples. We then continue the training on the same 50,000 training examples. However, only with a certain probabiliy was an image used for training. If an image was used for training, it was first perturbed using PGD before the forward pass of backpropagation.</div>

			<div>For example, when the probability is 60%, there is 60% chance that an image is perturbed and used for training. In other words, the model is trained on approximately 60% x 50,000 adversarial examples (after training on the 50,000 benign examples).</div>

			<div>Using this strategy, we trained six models. The first is a VGG16 model trained on the 50,000 benign examples. The other five models are created by continuing the training on 20%, 40%, 60%, 80%, and 100% x 50,000 adversarial examples.</div>

			<div><a href="https://github.com/terrancelaw/terrancelaw.github.io/blob/master/adversarial-training-experiments/transfer_from_clean/transfer_from_clean.ipynb" target="_blank">Jupyter Notebook for the Experiments</a></div>

		</div>
		<div class="vis-container">
			<span class="chart"><svg></svg></span>
			<span class="finding">
				
				<div>Key Observations:</div>

				<div>1. The model trained only on benign examples again has the lowest <span class="te-e">test_err</span> and the lowest <span class="adv-e">adv_err</span>.</div>

				<div>2. As the probability increases (training on more adversarial examples), <span class="te-e">test_err</span> decreases. This indicates that training on more adversarial examples helps the model generalize to benign examples.</div>

				<div>3. As the probability increases, <span class="adv-e">adv_err</span> mildly decreases.</div>

			</span>
		</div>
	</div>

	<div id="learning-from-adversarial">
		<div class="title">Strategy 3: Train on Adversarial Examples, Then on Benign Examples</div>
		<div class="introduction">
			
			<div>For the third strategy, we first trained a VGG16 model on the 50,000 adversarial examples. We then continue the training on the same 50,000 training examples. Again, an image was used for training only with a certain probabiliy. Contrary to strategy 2, the images used for training are benign.</div>

			<div>For example, when the probability is 60%, there is 60% chance that an image used for training, and this image is NOT perturbed. In other words, the model is trained on approximately 60% x 50,000 benign examples (after training on the 50,000 adversarial examples).</div>

			<div>Using this strategy, we trained six models. The first is a VGG16 model trained on the 50,000 adversarial examples. The other five models are created by continuing the training on 20%, 40%, 60%, 80%, and 100% x 50,000 benign examples.</div>

			<div><a href="https://github.com/terrancelaw/terrancelaw.github.io/blob/master/adversarial-training-experiments/transfer_from_adversarial/transfer_from_adversarial.ipynb" target="_blank">Jupyter Notebook for the Experiments</a></div>

		</div>
		<div class="vis-container">
			<span class="chart"><svg></svg></span>
			<span class="finding">
				
				<div>Key Observations:</div>

				<div>1. As the probability increases (training on more benign examples), <span class="te-e">test_err</span> decreases. Obviously, training on more benign examples helps the model generalize to benign examples.</div>

				<div>2. As the probability increases, <span class="adv-e">adv_err</span> also decreases, indicating that training on more benign examples helps the model generalize to adversarial examples.</div>

			</span>
		</div>
	</div>

	<div id="conclusion">
		<div class="title">Conclusion</div>
		<div class="introduction">
			
			<div>Unsurprisingly, the above findings suggest that training on adversarial examples and training on benign examples will both reduce errors on adversarial and benign examples.</div>

			<div>What is more interesting is that with weak a PGD attack (Ɛ=0.05 in our case), adversarial training only mildly improves model performance on adversarial examples (see Strategy 1 and Strategy 2).</div>

			<div>On the other hand, training on benign examples seems to do a pretty good job in improving model performance on adversarial examples (see Strategy 3). Indeed, the best <span class="te-e">test_err</span> and <span class="adv-e">adv_err</span> were achieved by the model trained solely on benign examples (see Strategy 1 and Strategy 2). Furthermore, different from adversarial training, it does not harm the performance on benign examples.</div>

			<div>The above results imply that with a weak PGD attack, training on benign examples alone is a better than adversarial training in improving adversarial robustness. However, does adversarial training with a stronger PGD attack (e.g., Ɛ=8.0) improves adversarial robustness? Future study will conduct further experiments.</div>

		</div>
	</div>

	<div id="individual-experiment">
		<div class="title">Visualizing Results for Each Experiment</div>
		<div class="introduction">

			<div>Here is an interactive visualization that allows you to explore <span class="tr-e">train_err</span>, <span class="te-e">test_err</span>, and <span class="adv-e">adv_err</span> per epoch for each of the 18 experiments.</div>

			<span class="dropdown-name">Strategy: </span>
			
			<span class="dropdown">
				<button class="btn btn-secondary dropdown-toggle" type="button" id="dropdownMenuButton" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Stochastic Adversarial Attack</button>
				<div class="dropdown-menu" aria-labelledby="dropdownMenuButton">
					<a class="dropdown-item" value="1">Stochastic Adversarial Attack</a>
					<a class="dropdown-item" value="2">Benign First, Then Adversarial</a>
					<a class="dropdown-item" value="3">Adversarial First, Then Benign</a>
		  		</div>
			</span>

			<span class="dropdown-name">Experiment: </span>

			<span class="dropdown">
				<button class="btn btn-secondary dropdown-toggle" type="button" id="dropdownMenuButton" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">0%</button>
				<div class="dropdown-menu" aria-labelledby="dropdownMenuButton">
					<a class="dropdown-item" value="1">0%</a>
					<a class="dropdown-item" value="2">20%</a>
					<a class="dropdown-item" value="3">40%</a>
					<a class="dropdown-item" value="4">60%</a>
					<a class="dropdown-item" value="5">80%</a>
					<a class="dropdown-item" value="6">100%</a>
		  		</div>
			</span>

		</div>
		<div class="vis-container">
			<span class="chart"><svg></svg></span>
			<span class="finding"></span>
		</div>
	</div>

	<div id="reference">
		<div>
			<a name="pgd"></a>[1] Towards Deep Learning Models Resistant to Adversarial Attacks (<a href="https://arxiv.org/pdf/1706.06083.pdf" target="_blank">https://arxiv.org/pdf/1706.06083.pdf</a>)
		</div>
	</div>

	<script src="js/start.js"></script>
	<script src="js/database.js"></script>
	<script src="js/stochasticView.js"></script>
	<script src="js/learningFromCleanView.js"></script>
	<script src="js/learningFromAdvView.js"></script>
	<script src="js/individualExperimentView.js"></script>
</body>